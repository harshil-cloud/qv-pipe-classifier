# Plan du Projet - QV-Pipe Classifier
### Objectif global :  
Am√©liorer la mAP de la classification multi-label des d√©fauts dans les vid√©os Quick-View (QV) via une approche optimis√©e en termes de donn√©es, de mod√®les, et de m√©thodes d'entra√Ænement.

---

##  **Vue d'ensemble**

L'objectif du projet est de cr√©er un mod√®le performant capable de classifier les d√©fauts pr√©sents dans les vid√©os QV-Pipe, un jeu de donn√©es d'inspection de canalisations.  
Pour ce faire, nous allons utiliser des techniques de deep learning adapt√©es aux vid√©os, tout en tenant compte des contraintes li√©es √† la taille des vid√©os et de leur nature d√©s√©quilibr√©e (long-tailed).

### **Contraintes principales :**  
- **Vid√©os lourdes :** Utilisation de techniques d'√©chantillonnage pour √©viter les mod√®les vid√©o trop co√ªteux.  
- **R√©partition multi-label et d√©s√©quilibr√©e des classes :** Prise en compte de ces caract√©ristiques avec des m√©thodes de perte adapt√©es (ASL, Class-Balanced Focal Loss).

---

##  **Objectifs du projet**

1. **Extraction des donn√©es :** Convertir les vid√©os en images utilisables pour entra√Æner un mod√®le efficace.
2. **Cr√©ation de super-images :** Construire des images composites 3√ó3 √† partir de frames √©chantillonn√©es pour augmenter la performance du mod√®le.
3. **Entra√Ænement du mod√®le :** Construire et entra√Æner plusieurs architectures deep learning adapt√©es √† la classification multi-label.
4. **Optimisation des hyperparam√®tres :** Utilisation de strat√©gies comme OneCycle, AdamW, et AMP pour un entra√Ænement rapide et stable.
5. **Am√©lioration des r√©sultats :** Utilisation de l'ensemble de plusieurs mod√®les pour maximiser la mAP.

---

##  **√âtapes du projet**

### **√âtape 1 : Pr√©paration des donn√©es offline**  
- **Objectif :** Transformer les vid√©os en images utiles et organiser les donn√©es pour l'entra√Ænement.
- **M√©thodes utilis√©es :**  
  - **Sampling uniforme :** Extraire 5 (√©tape 2) ou 9 (√©tape3) frames r√©guli√®rement espac√©es par vid√©o.  
  - **Nettoyage des donn√©es :**  
    - Supprimer les images floues (en utilisant la variance du Laplacien).
    - Supprimer les doublons (pHash).  
  - **Cr√©ation de splits stratifi√©s :**  
    - Split 5-fold multi-label en utilisant **iterative stratification** pour maintenir un √©quilibre des classes.


**D√©tails :**
- Nombre de vid√©os : 9 601 (55 heures de vid√©o).
- Chaque vid√©o contient des d√©fauts diff√©rents, avec des classes rares.

**Projets GitHub :**
- **[Decord](https://github.com/dmlc/decord)** - loader/lecture vid√©o ultra-rapide (FFmpeg/NV codecs). Parfait pour extraire des frames sans charger toute la vid√©o.  
- **[PyAV](https://github.com/PyAV-Org/PyAV)** - bindings FFmpeg en Python, flexible pour pipelines d‚Äôextraction personnalis√©s.  
- **[iterative-stratification](https://github.com/trent-b/iterative-stratification)** - cross-val **multi-label** (la stratification utilis√©e par les gagnants).  
- **[video2frame](https://github.com/jinyu121/video2frame)** - scripts simples d‚Äôextraction (uniforme, resize, multithread).  

---

### **√âtape 2 : Baseline frame-wise**  
- **Objectif :** √âtablir une premi√®re baseline de classification en utilisant 5 frames par vid√©o, trait√©es ind√©pendamment puis fusionn√©es au niveau des pr√©dictions.

- **Pr√©traitement en ligne (DataLoader) :**  
  - **Resize** des frames √† la r√©solution attendue par le backbone.  
  - **Conversion en tenseur PyTorch** (`ToTensor`).  
  - **Normalisation ImageNet** avec les statistiques standards :  
    - mean = [0.485, 0.456, 0.406]  
    - std = [0.229, 0.224, 0.225]  
  *(Ces op√©rations sont appliqu√©es √† la vol√©e, et non en pr√©traitement offline.)*

- **Mod√®le utilis√© :**  
  - **CNN simple** bas√© sur **ResNet-18** (pr√©-entra√Æn√© ImageNet) ou **TResNet** pour une baseline multi-label plus robuste.

- **Fusion vid√©o :**  
  - Passage des **5 frames individuellement** dans le backbone.  
  - Agr√©gation par **moyenne des logits** pour obtenir une pr√©diction unique par vid√©o.

- **Fonctions de perte :**  
  - **BCE (Binary Cross-Entropy)** comme base simple.  
  - **ASL (Asymmetric Loss)** recommand√©e pour g√©rer l‚Äôextr√™me d√©s√©quilibre des classes (d√©fauts rares).

- **Strat√©gie d‚Äôoptimisation :**  
  - **AdamW** comme optimiseur principal.  
  - Scheduler **OneCycleLR** pour une mont√©e rapide puis d√©croissance contr√¥l√©e du taux d‚Äôapprentissage.  
  - Utilisation de **l‚ÄôAMP (Automatic Mixed Precision)** pour r√©duire l‚Äôutilisation m√©moire et acc√©l√©rer l'entra√Ænement.

**Projets GitHub utiles :**
- **timm** ‚Äì vaste biblioth√®que de backbones (ResNet/TResNet) et outils d‚Äôentra√Ænement (optimiseurs, schedulers, EMA).  
- **TResNet (MIIL)** ‚Äì mod√®le multi-label performant, utilis√© comme baseline dans plusieurs solutions gagnantes.  


### **√âtape 3 : Super-images 3√ó3**  
- **Objectif :** Exploiter davantage d‚Äôinformation temporelle en assemblant 9 frames d‚Äôune m√™me vid√©o en une grille 3√ó3 (super-image), puis entra√Æner des mod√®les plus puissants pour am√©liorer la mAP globale.

- **Construction des super-images :**  
  - S√©lection de **9 frames** r√©guli√®rement espac√©es par vid√©o.  
  - Assemblage des 9 images en une **grille 3√ó3** (super-image), conform√©ment √† l‚Äôapproche SIFAR.  
  - Sauvegarde des super-images sous forme d‚Äôimages RGB standard (JPEG/PNG).

- **Pr√©traitement en ligne (DataLoader) :**  
  - **Resize** √† la r√©solution requise par le backbone (selon ConvNeXt/NFNet/TResNet-XL).  
  - **Conversion en tenseur PyTorch**.  
  - **Normalisation ImageNet** (mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]).  
  - Ces op√©rations sont appliqu√©es √† la vol√©e, et non int√©gr√©es dans les super-images stock√©es sur disque.

- **Mod√®les utilis√©s :**  
  - Backbones haute performance issus de `timm`, tels que **ConvNeXt**, **NFNet**, **EfficientNet**, ou **TResNet-XL**.  
  - Ajout possible d‚Äôune t√™te multi-label avanc√©e, par exemple **ML-Decoder**, qui am√©liore la s√©paration des classes longues queues.

- **Fonctions de perte :**  
  - **ASL (Asymmetric Loss)** pour g√©rer le d√©s√©quilibre s√©v√®re du dataset.  
  - **Class-Balanced Focal Loss (CB-Focal)** comme alternative pour les classes tr√®s rares.

- **Augmentations sp√©cifiques :**  
  - **Horizontal Flip** l√©ger (probabilit√© mod√©r√©e).  
  - **Tile Shuffle** : permutation l√©g√®re et contr√¥l√©e des tuiles constitutives de la super-image, une technique utilis√©e dans plusieurs solutions performantes pour am√©liorer la robustesse sans alt√©rer la texture des d√©fauts.

**Projets GitHub utiles :**
- **IBM/sifar-pytorch** ‚Äì impl√©mentation compl√®te du concept ‚Äúsuper-image‚Äù (assemblage 3√ó3 ou 4√ó4 + entra√Ænement d‚Äôun classifieur).  
- **timm** ‚Äì backbones modernes (**ConvNeXt**, **NFNet**, **EfficientNet**), schedulers (**OneCycle**), normalisation, AMP et EMA int√©gr√©s.  
- **ML-Decoder** ‚Äì t√™te multi-label performante, compatible avec un backbone timm (approche utilis√©e par TResNet-XL).  
- **ASL (Asymmetric Loss)** ‚Äì impl√©mentation officielle MIIL adapt√©e au multi-label d√©s√©quilibr√©.  
 

---

### **√âtape 4 : Recette d‚Äôentra√Ænement stable**  
- **Objectif :** Optimiser l'entra√Ænement avec une strat√©gie stable et rapide.
- **M√©thode :**  
  - **AdamW** comme optimiseur.  
  - **OneCycleLR** pour ajuster le taux d'apprentissage.  
  - **AMP (half-precision)** pour acc√©l√©rer l‚Äôentra√Ænement en utilisant une pr√©cision r√©duite.
  - **EMA (Exponential Moving Average)** des poids pour am√©liorer la stabilit√©.
  - **Early stopping** sur la mAP pour arr√™ter l'entra√Ænement si la performance stagne.

**Projets GitHub :**
- **[timm (trainer \+ schedulers)](https://github.com/huggingface/pytorch-image-models)** ‚Äî **AdamW \+ OneCycle \+ AMP (fp16)**, **EMA**, callbacks et scripts reproductibles.  
- **[Class-Balanced Loss (effective number)](https://github.com/vandit15/Class-balanced-loss-pytorch)** ‚Äî impl√©mentations PyTorch pr√™tes pour **CB-Focal** (alternative/benchmark d‚ÄôASL).  

---

### **√âtape 5 : Ensemble de mod√®les**  
- **Objectif :** Am√©liorer la performance finale en combinant plusieurs mod√®les.
- **M√©thode :**  
  - **Ensemble des pr√©dictions :** Moyenne des pr√©dictions sur 5 folds (intra-mod√®le).
  - **Pond√©ration des mod√®les :** Utilisation de mod√®les diff√©rents (par exemple, NFNet, EffNet, ConvNeXt) et pond√©ration en fonction de leur mAP de validation.
  - **Post-traitement minimal :** Application d'une r√®gle de seuil (**ZC > 0.9 ‚Üí ZC = 1**) pour un l√©ger gain.

**Projets GitHub :**
- **[MMAction2](https://github.com/open-mmlab/mmaction2)** ‚Üí pour comparer plus tard avec un run Video Swin-B (Top2).

---

##  **Technologies et Frameworks**

- **Frameworks principaux :**
  - **PyTorch** pour le deep learning.
  - **timm (PyTorch Image Models)** pour les backbones pr√©-entra√Æn√©s.
  - **Decord** et **PyAV** pour l'extraction rapide des frames.
  
- **Pertes utilis√©es :**
  - **BCE** pour la baseline.
  - **ASL (Asymmetric Loss)** et **Class-Balanced Focal Loss** pour le multi-label long-tailed.
  
- **Optimisation :**
  - **AdamW**, **OneCycleLR**, **AMP**, **EMA**, **gradient accumulation**.

---

## ‚ö†Ô∏è **Consid√©rations importantes**

- **Donn√©es d√©s√©quilibr√©es :** QV-Pipe est un dataset **multi-label et long-tailed**, avec des classes rares qui n√©cessitent une gestion particuli√®re des pertes.
- **Super-images :** La transformation vid√©o en super-image 3√ó3 est **la m√©thode la plus performante** avec un gain net de 10 mAP.
- **Ensemble de mod√®les :** Un **ensemble simple** (moyenne des logits) sur 2-3 mod√®les diff√©rents peut apporter un gain de **+2 √† +3 mAP**.

---

##  **Astuces pour am√©liorer la mAP**

- **Pertes :** Tester **ASL vs CB-Focal** et choisir celle qui maximise la mAP.
- **Sampling :** Toujours utiliser un **sampling uniforme** pour extraire les frames (√©viter les biais).
- **Mod√®les :** Privil√©gier les backbones **pr√©-entra√Æn√©s sur ImageNet** (ConvNeXt, NFNet, EfficientNet).
- **Ensemble :** La combinaison de **5 folds** et de mod√®les diff√©rents (NFNet, EffNet, ConvNeXt) donne des r√©sultats solides.

---

## üîó **Liens vers des ressources GitHub utiles**

- [Decord](https://github.com/dmlc/decord) - loader/lecture vid√©o ultra-rapide (FFmpeg/NV codecs). Parfait pour extraire des frames sans charger toute la vid√©o.  
- [PyAV](https://github.com/PyAV-Org/PyAV) - Traitement vid√©o en Python.  
- [iterative-stratification](https://github.com/trent-b/iterative-stratification) - Stratification pour cross-validation multi-label.  
- [video2frame](https://github.com/jinyu121/video2frame) - Outils simples pour extraire des frames de vid√©os.  
- [timm (PyTorch Image Models)](https://github.com/huggingface/pytorch-image-models) - Collection de mod√®les pr√©-entra√Æn√©s.  
- [TResNet (MIIL)](https://github.com/Alibaba-MIIL/TResNet) - Architecture multi-label performante et l√©g√®re.  
- [ML-Decoder (head multi-label)](https://github.com/Alibaba-MIIL/ML_Decoder) - T√™te multi-label pour mod√®les.
- [ASL (Asymmetric Loss)](https://github.com/Alibaba-MIIL/ASL) - Perte Asymmetric Loss pour multi-label long-tailed.  
- [Class-Balanced Loss](https://github.com/vandit15/Class-balanced-loss-pytorch) - Perte Class-Balanced Focal.  
